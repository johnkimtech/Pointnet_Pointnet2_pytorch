{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Convert STL to TXT files and their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STL_ROOT = 'data/Foot_Last_stl/STL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from data_utils import helpers\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_files_list = [] # input\n",
    "txt_files_list = [] # output\n",
    "\n",
    "for root, dirs, files in os.walk(STL_ROOT, topdown=False):\n",
    "    txt_root = re.sub('(?i)stl', 'txt', root)\n",
    "    Path(txt_root).mkdir(parents=True, exist_ok=True)\n",
    "    stl_files_list += [os.path.join(root, file) for file in files]\n",
    "\n",
    "txt_files_list = list(map(lambda stl_file: re.sub('(?i)stl', 'txt', stl_file), stl_files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_FILE = \"data/Foot_Last_stl/STL/Foot_stl/Foot_Right_stl/202212071755ra.stl\n",
    "# helpers.stl_to_xyz_with_normals_vectorized(TEST_FILE, 'temp/c.txt', stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "DATA_ROOT = '/home/hoang/Projects/Pointnet_Pointnet2_pytorch/data/modelnet40_withFootLast_AllLeft/'\n",
    "subdirs = ['foot', 'last']\n",
    "EXT = '.txt'\n",
    "DOWNSAMPLING = 10\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # CONVERT stl to txt and (optionally) downsample\n",
    "# for stl, txt in tqdm(zip(stl_files_list, txt_files_list), total=len(txt_files_list)):\n",
    "#     helpers.stl_to_xyz_with_normals_vectorized(stl, txt, stride=DOWNSAMPLING)\n",
    "\n",
    "# Rename for easy process\n",
    "# for subdir in subdirs:\n",
    "#     for idx, fn in enumerate(glob.glob(os.path.join(DATA_ROOT, subdir, f'*{EXT}'))):\n",
    "#         os.rename(fn, os.path.join(DATA_ROOT, subdir, f'{subdir}_{idx}{EXT}'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, please move move and symbolically link files into a new directory so we can make use of provided ModelNetDataset class by the authors. After, we'll create some text files containing labels and files list for train / test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import random\n",
    "\n",
    "DATA_ROOT = '/home/hoang/Projects/Pointnet_Pointnet2_pytorch/data/modelnet40_withFootLast/'\n",
    "TRAIN_SPLIT = .8\n",
    "SEED = 42\n",
    "EXT = '.txt'\n",
    "\n",
    "files_list = []\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "random.seed(SEED)\n",
    "for root, subdirs, _ in os.walk(DATA_ROOT):\n",
    "    subdirs =[ sd for sd in subdirs if not sd.startswith('.')]\n",
    "    for sdir in subdirs:\n",
    "        all_files = [fn.replace(DATA_ROOT, '') for fn in glob.glob(os.path.join(DATA_ROOT, sdir, \"*.txt\"))]\n",
    "        files_list += all_files\n",
    "        all_files = [os.path.splitext(os.path.basename(file))[0] for file in all_files]\n",
    "        random.shuffle(all_files)\n",
    "        n_train = int(TRAIN_SPLIT*len(all_files))\n",
    "        train_files += all_files[:n_train]\n",
    "        test_files += all_files[n_train:]\n",
    "\n",
    "    if len(subdirs) > 0:\n",
    "        # write all shape classes\n",
    "        with open(os.path.join(DATA_ROOT, 'modelnet40_shape_names.txt'), 'w') as ff:\n",
    "            ff.writelines(\"\\n\".join(subdirs))\n",
    "\n",
    "# write all shape files\n",
    "with open(os.path.join(DATA_ROOT, 'filelist.txt'), 'w') as ff:\n",
    "    ff.writelines(\"\\n\".join(set(files_list)))\n",
    "\n",
    "\n",
    "\n",
    "# write training files. I used same names as modelnet40 to make use of prewritten classes easily\n",
    "with open(os.path.join(DATA_ROOT, 'modelnet40_train.txt'), 'w') as ff:\n",
    "    ff.writelines(\"\\n\".join(set(train_files)))\n",
    "\n",
    "# write test files\n",
    "with open(os.path.join(DATA_ROOT, 'modelnet40_test.txt'), 'w') as ff:\n",
    "    ff.writelines(\"\\n\".join(set(test_files)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
